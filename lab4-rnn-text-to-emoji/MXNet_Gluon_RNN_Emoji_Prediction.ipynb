{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Recurrent Neural Network (RNN) with Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Objectives\n",
    "\n",
    "1. End to end process of training a deep learning model\n",
    "2. Text data processing - Tokenization, clipping, vocabulary indexes, embedding \n",
    "3. Recurrent Neural Network (RNN) with MXNet Gluon\n",
    "4. Inference on text data with MXNet Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Problem definition\n",
    "\n",
    "1. Given an input text, ex: \"I love my mother\", predict an EMOJI, ex: ‚ù§Ô∏è\n",
    "2. In this problem, we restrict our setting to predict one of **5 EMOJIs**:\n",
    "    * sad - üòû\n",
    "    * love - ‚ù§Ô∏è\n",
    "    * happy - üòÑ\n",
    "    * fork & knife - üç¥\n",
    "    * baseball - ‚öæ\n",
    "3. This is a `Text Classification` problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if matplotlib and emoji is not installed.\n",
    "# !pip install matplotlib\n",
    "# !pip install emoji\n",
    "\n",
    "# Uncomment the following lines if gluon-nlp, spacy is not installed.\n",
    "# !pip install gluon-nlp\n",
    "# !pip install spacy -U --quiet\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required modules from MXNet and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import csv\n",
    "import emoji\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MXNet Context. Use mx.cpu() for CPU. Use mx.gpu(0) for 1 GPU\n",
    "context = mx.cpu() #mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Load Pretrained wikitext-2 Language Model\n",
    "\n",
    "We use a pretrained model on wikitext-2 dataset [6]. Specifically, we use Vocabulary, Language Model i.e., Embeddings and Encodings (LSTM weights) based on wikitext-2 dataset.\n",
    "\n",
    "**Intuition:** Using pretrained language model weights is a common approach for semi-supervised learning in NLP. In order to do a good job with large language modeling on a large corpus of text, our model must learn representations that contain information about the structure of natural language. Intuitively, by starting with these good features, vs random features, we‚Äôre able to converge faster upon a good model for our downsteam task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_name = 'standard_lstm_lm_200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=True,\n",
    "                                      ctx=context,\n",
    "                                      dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Data collection and data preparation\n",
    "\n",
    "## 3.1 Data collection\n",
    "\n",
    "* Load Emojify dataset\n",
    "* This is a tiny dataset (X, Y) where:\n",
    "       * X contains 127 sentences (strings) \n",
    "       * Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "    ![Emoji Dataset](../assets/emoji_data_set.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filename):\n",
    "    \"\"\"Utility to read CSV data from given filename\n",
    "    \"\"\"\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "    return phrase, emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('./data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('./data/test_emoji.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mx.gluon.data.dataset.ArrayDataset(X_train, Y_train)\n",
    "test_dataset = mx.gluon.data.dataset.ArrayDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\" Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Process data and prepare dataset\n",
    "\n",
    "* Tokenize using spaCy- Extract words, punctuation marks from review text\n",
    "* Convert each token to an index in the vocabulary. Vocabulary is obtained from wikitext2\n",
    "* Prepare data iterators that can iterate on training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Train dataset...\n",
      "Preparing Test dataset...\n",
      "Data is ready!!!\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy English (en) tokenizer on input sentences to get tokens(words and punctuation marks)\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# Clip sentences to be max 500 tokens\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    1. Tokenize - Extract words, punctuation marks from review text.\n",
    "    2. Convert each token to an index in the vocabulary.\n",
    "    \"\"\"\n",
    "    data, label = x\n",
    "\n",
    "    # Tokenize the data\n",
    "    tokenized_data = tokenizer(data)\n",
    "    # Clip the tokens\n",
    "    tokenized_clipped_data = length_clip(tokenized_data)\n",
    "    # Get vocabulary indexes for the tokens. Use pre-loaded 'vocab'.\n",
    "    data = vocab[tokenized_clipped_data]\n",
    "\n",
    "    return data, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    \n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "print(\"Preparing Train dataset...\")\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "\n",
    "print(\"Preparing Test dataset...\")\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)\n",
    "\n",
    "print(\"Data is ready!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualize sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data -  they are so kind and friendly ‚ù§Ô∏è\n",
      "Processed Data -  ([56, 36, 143, 1735, 6, 4903], '0')\n"
     ]
    }
   ],
   "source": [
    "# Raw data - Text and label\n",
    "# Print a Test data\n",
    "index = 77\n",
    "print(\"Raw Data - \", X_train[index], label_to_emoji(Y_train[index]))\n",
    "\n",
    "# Processed data - Text is tokenized and converted to index of vocabulary \n",
    "print(\"Processed Data - \", train_dataset[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prepare MXNet Dataloader\n",
    "\n",
    "You know have a dataset for training data and test data. But, you need to feed this data into the neural network for training the model. You use data loader!\n",
    "\n",
    "1. Data Loader reads the data from dataset and feed them as batches to the neural network for model training\n",
    "2. Data Loader is fast, efficient and can run multiple processes in parallel to feed the batches of data for model training\n",
    "\n",
    "Below, we prepare data loaders for training data and test data:\n",
    "1. We use 4 workers to load the data in parallel\n",
    "2. Randomly shuffle the training data (Intuition: Without shuffling, you will keep feeding the training data in same order leading to neural network memorizing rather than learn important feature)\n",
    "3. Feed 16 sentences per batch during the model training\n",
    "\n",
    "Few important notes to consider in Text data modelling:\n",
    "\n",
    "* Input sentences can be of different lengths.\n",
    "* Use FixedBucketSampler, which assigns each data sample to a fixed bucket based on its length.\n",
    "* Batchify function (batchify) is applied on all the samples as the loaders read the batches.\n",
    "* We apply *Pad* for padding smaller length sequence to max length sequence in the bucket.\n",
    "* We apply *Stack* for stacking data, label, data_length i.e., [sentence, sentiment label, sentence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "bucket_num, bucket_ratio = 5, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=132, batch_num=10\n",
      "  key=[2, 4, 6, 8, 10]\n",
      "  cnt=[9, 61, 41, 15, 6]\n",
      "  batch_size=[16, 16, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader():\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn, num_workers=4)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn, num_workers=4)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Define the Neural Network\n",
    "\n",
    "* **Embedding, LSTM Layer:** To use pre-trained weights, we base our network on the Language Model Network (Embedding -> LSTM). \n",
    "* **Mean Pooling Layer:** We have multiple words input (reviews) and one output (sentiment). Hence, we average(mean) states across all time steps into one value.\n",
    "* **Dense Layer:** To generate the final output\n",
    "\n",
    "<img src=\"../assets/emoji_model.png\" width=\"400\" height=\"1100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition - Embedding\n",
    "\n",
    "<img src=\"../assets/emoji_embedding.png\" width=\"1000\" height=\"1100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolingLayer(gluon.HybridBlock):\n",
    "    \"\"\"A block for mean pooling of encoder features\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(MeanPoolingLayer, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length):\n",
    "        masked_encoded = F.SequenceMask(data,\n",
    "                                        sequence_length=valid_length,\n",
    "                                        use_sequence_length=True)\n",
    "        agg_state = F.broadcast_div(F.sum(masked_encoded, axis=0),\n",
    "                                    F.expand_dims(valid_length, axis=1))\n",
    "        return agg_state\n",
    "\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    \"\"\"Network for sentiment analysis.\"\"\"\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None # will set with lm embedding later\n",
    "            self.encoder = None # will set with lm encoder later\n",
    "            self.agg_layer = MeanPoolingLayer()\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                # 5 possible emojis, hence, output 5 values from last layer.\n",
    "                self.output.add(gluon.nn.Dense(5, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): \n",
    "        embedded = self.embedding(data)\n",
    "        encoded = self.encoder(embedded)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Initialize the parameters in the Neural Network\n",
    "\n",
    "1. Initialize Embedding and Encoding layer with pre-trained weights from \"wikitext-2\" dataset (see Step 1)\n",
    "2. Use Xavier initializer to randomly initialize last Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2)\n",
      "  (agg_layer): MeanPoolingLayer(\n",
      "  \n",
      "  )\n",
      "  (output): HybridSequential(\n",
      "    (0): Dense(None -> 5, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SentimentNet()\n",
    "\n",
    "# Use Pretrained Embeddings from wikitext-2\n",
    "net.embedding = lm_model.embedding\n",
    "\n",
    "# Use Pretrained Encoder states (LSTM) from wikitext-2\n",
    "net.encoder = lm_model.encoder\n",
    "\n",
    "net.hybridize()\n",
    "\n",
    "# Random initialize the last Dense Laywer\n",
    "net.output.initialize(mx.init.Xavier(), ctx=context)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 10\n",
    "grad_clip = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    print('Begin Testing...')\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "        # Step 1: Prepare data\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        \n",
    "        # Step 2: Forward pass\n",
    "        output = net(data, valid_length)\n",
    "        \n",
    "        # Step 3: Calculate loss\n",
    "        L = loss(output, label)\n",
    "        \n",
    "        # Step 4: Statistics - Keeping moving average loss and accuracy\n",
    "        pred =  nd.argmax(output, axis=1) #(output > 0.5).reshape(-1)\n",
    "        \n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train the Model\n",
    "\n",
    "### Training loop\n",
    "\n",
    "1. Take a batch of training data from data loader\n",
    "2. Do the forward pass (prediction)\n",
    "3. Calculate the loss (error)\n",
    "4. Do the backward pass (gradient - change required to reduce the loss)\n",
    "5. Use Trainer with optimizer to make the updates (change in weights)\n",
    "6. Continue with Step 1 with new batch of data\n",
    "\n",
    "**NOTE:** Softmax cross entropy loss is used to measure error in multi class classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    # Use Follow the Moving Leader Optimizer - [7]\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'ftml',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    parameters = net.collect_params().values()\n",
    "    print(\"Training the Emoji Prediction Model...\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        print(\"[Epoch - {}]\".format(epoch))\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            with autograd.record():\n",
    "                # Step 1: Forward pass\n",
    "                output = net(data.as_in_context(context).T,\n",
    "                             length.as_in_context(context)\n",
    "                                   .astype(np.float32))\n",
    "                # Step 2: Calculate Loss\n",
    "                L = L + loss(output, label.as_in_context(context)).mean()\n",
    "            \n",
    "            # Step 3: Backward pass\n",
    "            L.backward()\n",
    "            \n",
    "            # Step 3.1: Clip gradient - Avoid gradient explosion\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            \n",
    "            # Step 4: Do parameter updates\n",
    "            trainer.step(1)\n",
    "            \n",
    "            # For epoch statistics - Loss and data sample count\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            epoch_L += L.asscalar()\n",
    "    \n",
    "        print('Train Avg Loss {:.6f}'.format(epoch_L / epoch_sent_num))\n",
    "        \n",
    "        # Step 5: Evaluation after each epoch\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "        print('Test Acc {:.2f}, Test Avg Loss {:.6f}'.format(test_acc, test_avg_L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Emoji Prediction Model...\n",
      "[Epoch - 0]\n",
      "Train Avg Loss 0.298171\n",
      "Begin Testing...\n",
      "Test Acc 0.23, Test Avg Loss 1.555015\n",
      "[Epoch - 1]\n",
      "Train Avg Loss 0.276428\n",
      "Begin Testing...\n",
      "Test Acc 0.38, Test Avg Loss 1.507533\n",
      "[Epoch - 2]\n",
      "Train Avg Loss 0.255712\n",
      "Begin Testing...\n",
      "Test Acc 0.38, Test Avg Loss 1.454944\n",
      "[Epoch - 3]\n",
      "Train Avg Loss 0.231783\n",
      "Begin Testing...\n",
      "Test Acc 0.38, Test Avg Loss 1.394452\n",
      "[Epoch - 4]\n",
      "Train Avg Loss 0.205929\n",
      "Begin Testing...\n",
      "Test Acc 0.48, Test Avg Loss 1.328168\n",
      "[Epoch - 5]\n",
      "Train Avg Loss 0.173582\n",
      "Begin Testing...\n",
      "Test Acc 0.57, Test Avg Loss 1.262833\n",
      "[Epoch - 6]\n",
      "Train Avg Loss 0.139932\n",
      "Begin Testing...\n",
      "Test Acc 0.59, Test Avg Loss 1.196853\n",
      "[Epoch - 7]\n",
      "Train Avg Loss 0.109334\n",
      "Begin Testing...\n",
      "Test Acc 0.59, Test Avg Loss 1.141708\n",
      "[Epoch - 8]\n",
      "Train Avg Loss 0.078590\n",
      "Begin Testing...\n",
      "Test Acc 0.61, Test Avg Loss 1.101847\n",
      "[Epoch - 9]\n",
      "Train Avg Loss 0.056978\n",
      "Begin Testing...\n",
      "Test Acc 0.64, Test Avg Loss 1.077297\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(net, context, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 - Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚öæ'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['He', 'likes', 'playing', 'baseball']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‚ù§Ô∏è'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['I', 'love', 'my', 'mother']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üç¥'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['Lets', 'have', 'food', 'in', 'indian', 'restaurant']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üòû'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['I', 'am', 'not', 'feeling', 'good']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üòÑ'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob1 = net(\n",
    "            mx.nd.reshape(\n",
    "            mx.nd.array(vocab[['He', 'is', 'funny']], ctx=context),\n",
    "            shape=(-1, 1)), mx.nd.array([4], ctx=context)).sigmoid()\n",
    "nd.argmax(prob1, axis=1).asscalar()\n",
    "label_to_emoji(int(nd.argmax(prob1, axis=1).asscalar()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "In this lab, we learnt about:\n",
    "\n",
    "1. End to end process of training a deep learning model for Text Data usecase\n",
    "2. Gluon Dataset, Data Loader and text data processing\n",
    "3. Spacy tokenizer, Gluon NLP bucketizer, Vocabulary from WikiText2 dataset\n",
    "4. Pre-trained Embedding and Encoding weights (WikiText2 dataset) using easy to use Gluon-NLP toolkit\n",
    "5. Gluon Blocks - Embedding, LSTM\n",
    "6. Loss function - gluon.loss.SoftmaxCrossEntropyLoss\n",
    "7. Trainer and optimizer - gluon.Train and FTML (Follow the moving leader optimizer)\n",
    "8. Forward -> Loss -> Backward -> Update -> Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "Problem statement and dataset is borrowed from [Coursera deeplearning.ai Sequence Models lecture](https://www.coursera.org/learn/nlp-sequence-models/home/welcome)\n",
    "\n",
    "# References\n",
    "\n",
    "1. http://mxnet.incubator.apache.org/\n",
    "2. https://gluon-nlp.mxnet.io\n",
    "3. https://spacy.io/usage/\n",
    "4. https://gluon-nlp.mxnet.io/examples/sentiment_analysis/sentiment_analysis.html\n",
    "5. http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "6. https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset\n",
    "7. https://mxnet.incubator.apache.org/api/python/optimization/optimization.html#mxnet.optimizer.FTML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
